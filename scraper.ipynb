{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "#from logging import exception\n",
    "\n",
    "class Scrape:\n",
    "    \"\"\" This class contains the blueprints for a webscraper\n",
    "\n",
    "    This class will access a website and gather information\n",
    "    on different products listed and will store it in a \n",
    "    dictionary.\n",
    "\n",
    "    Attributes:\n",
    "        url : The website that will be webscraped.\n",
    "        driver: The driver that will be used for webscraping.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url = 'https://www.asos.com/men/sale/cat/?cid=8409&nlid=mw|sale|shop+sale+by+product|sale+view+all'\n",
    "        self.driver = Chrome(ChromeDriverManager().install())\n",
    "        self.driver.get(self.url)\n",
    "        self.driver.maximize_window()\n",
    "        #self.shoes = []\n",
    "        #self.links = []\n",
    "        self.DATABASE_TYPE = 'postgresql'\n",
    "        self.DBAPI = 'psycopg2'\n",
    "        self.ENDPOINT = 'aicoredb.cjfn9z1woyjk.us-east-1.rds.amazonaws.com' # Change it for your AWS endpoint\n",
    "        self.USER = 'postgres'\n",
    "        self.PASSWORD = 'worm1997'\n",
    "        self.PORT = 5432\n",
    "        self.DATABASE = 'postgres'\n",
    "        self.engine = create_engine(f\"{self.DATABASE_TYPE}+{self.DBAPI}://{self.USER}:{self.PASSWORD}@{self.ENDPOINT}:{self.PORT}/{self.DATABASE}\")\n",
    "        \n",
    "\n",
    "\n",
    "#     def cookies(self):\n",
    "#         \"\"\"Makes the webscraper accept cookies on the website otherwise \n",
    "#         if cookies do not appear then it prints 'cookies are not \n",
    "#         found'.\"\"\"\n",
    "#         xpath = '//*[@id=\"onetrust-accept-btn-handler\"]'\n",
    "\n",
    "#         try:\n",
    "#             time.sleep(2)\n",
    "#             WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "#             self.driver.find_element(By.XPATH, xpath).click()\n",
    "#         except TimeoutException:\n",
    "#             print(\"Cookies not found\")\n",
    "\n",
    "\n",
    "#     def clothe_container(self):\n",
    "#         \"\"\"Makes the webscraper find the container with all the products\n",
    "#            and then stores the links for all the products in a list. The \n",
    "#            webscraper then checks out the links and stores information \n",
    "#            about these products in a dictionary. The information stored\n",
    "#            are the names, price, URL, product ID for the clothes. A UUID4\n",
    "#            is also generated for each item of clothing. \"\"\"\n",
    "#         clothes = self.driver.find_element(By.XPATH, '//*[@id=\"plp\"]/div/div[1]/div[2]/div/div[1]/section')\n",
    "#         clothe_list = clothes.find_elements(By.XPATH, './article')\n",
    "#         link_list = []\n",
    "\n",
    "\n",
    "#         for clothe in clothe_list:\n",
    "#             a_tag = clothe.find_element(by = By.TAG_NAME, value = 'a')\n",
    "#             link = a_tag.get_attribute('href')\n",
    "#             link_list.append(link)\n",
    "\n",
    "#         print(f'There are {len(link_list)} clothes in this page')\n",
    "#         #print(link_list)\n",
    "\n",
    "#         clothe_dict = {'Clothe Name: ': [], 'Price: ': [], 'URL: ': [],'Image: ': [], 'Unique ID: ': [], 'UUID 4: ': [] }\n",
    "#         for i in link_list[0:5]:\n",
    "#             self.driver.get(i)\n",
    "#             time.sleep(2)\n",
    "#             clothe_dict['URL: '].append(str(i))\n",
    "\n",
    "#             try:\n",
    "#                 Clothe_name = self.driver.find_element(By.XPATH, '//*[@id=\"aside-content\"]/div[1]/h1').text\n",
    "#                 clothe_dict['Clothe Name: '].append(str(Clothe_name))\n",
    "#             except NoSuchElementException:\n",
    "#                 clothe_dict['Clothe Name: '].append('N/A')\n",
    "\n",
    "#             try:\n",
    "#                 Price = self.driver.find_element(By.XPATH, '//*[@id=\"product-price\"]/div/span[2]/span[4]/span[1]').text\n",
    "#                 clothe_dict['Price: '].append(str(Price))\n",
    "#             except NoSuchElementException:\n",
    "#                 clothe_dict['Price: '].append('N/A')\n",
    "\n",
    "#             try:\n",
    "#                 Pic = self.driver.find_element(By.XPATH, '//*[@id=\"product-gallery\"]/div[2]/div[2]/div[2]')\n",
    "#                 clothe_dict['Image: '].append(str(Pic))\n",
    "#             except NoSuchElementException:\n",
    "#                 clothe_dict['Image: '].append('N/A')\n",
    "\n",
    "#             try:\n",
    "#                 Unique_ID = self.driver.find_element(By.XPATH, '//*[@id=\"product-details-container\"]/div[2]/div[1]/p').text\n",
    "#                 clothe_dict['Unique ID: '].append(str(Unique_ID))\n",
    "#             except NoSuchElementException:\n",
    "#                 clothe_dict['Unique ID: '].append('N/A')\n",
    "\n",
    "\n",
    "#             unique = uuid.uuid4()\n",
    "#             clothe_dict['UUID 4: '].append(str(unique))\n",
    "\n",
    "\n",
    "#         df = pd.DataFrame(clothe_dict)\n",
    "#         #print(df)\n",
    "#         print(clothe_dict)\n",
    "#         with open('data.json', 'w') as fp:\n",
    "#             json.dump(clothe_dict, fp, indent = 4)\n",
    "\n",
    "#         #self.driver.quit() # Close the browser when you finish\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\": #it only runs if this file is run directly rather than on any import \n",
    "#     bot = Scrape()\n",
    "#     bot.cookies()\n",
    "#     bot.clothe_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 99.0.4844\n",
      "Get LATEST chromedriver version for 99.0.4844 google-chrome\n",
      "Driver [/home/abdul/.wdm/drivers/chromedriver/linux64/99.0.4844.51/chromedriver] found in cache\n",
      "/tmp/ipykernel_68956/736718978.py:32: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self.driver = Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x7f7635347610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot = Scrape()\n",
    "bot.engine.connect()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81ea520512955b225de66c05f8314d1a1a3f3e5b9755019e67431f7dc174bbd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
